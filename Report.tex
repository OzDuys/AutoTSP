\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{bm}
\usepackage{standalone}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{float}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{tikz}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{booktabs}

\newtheorem{definition}{Definition}

\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
\addbibresource{references.bib}

\usetikzlibrary{shapes.geometric, arrows, positioning, calc}
\usetikzlibrary{trees}
\usetikzlibrary{arrows.meta, positioning}

\geometry{a4paper, margin=1in}

\title{AutoTSP: Per-Instance Algorithm Selection for the Traveling Salesman Problem}
\author{Oscar Duys\thanks{Experimental code: \url{https://github.com/OzDuys/AutoTSP}}}
\date{November 2025}

\begin{document}

\maketitle
\vspace{2em}
\begin{center}
    \includegraphics[width=0.45\linewidth]{UCT Logo.png}
\end{center}
\vspace{2em}

\begin{abstract}
The Traveling Salesman Problem (TSP) is a standard testbed for
combinatorial optimisation, with a large ecosystem of exact algorithms,
heuristics, and metaheuristics. In practice, no single algorithm is
best for all instances: performance depends strongly on problem size,
metric structure, and geometric layout. This report studies this
instance–dependence systematically and uses it to build \emph{AutoTSP},
a simple per-instance algorithm selection system for the TSP.

The work starts from a heterogeneous benchmark of synthetic and
real-world TSP instances, together with a portfolio of exact methods,
constructive heuristics, local search, and metaheuristics. Algorithms
are evaluated under a fixed wall-clock budget using a penalised
multi-objective metric that combines tour quality and runtime and
treats timeouts as expensive failures. This analysis reveals pronounced
shifts in the quality–time Pareto front across instance families and
sizes, confirming that different solvers dominate in different regions
of the instance space.

Building on this, AutoTSP uses cheap instance features to select a
single solver per instance. Two selectors are considered: a transparent
rule-based policy and a learning-based Random Forest trained on
benchmark logs. On a held-out test set the Random Forest selector
matches an oracle choice on most instances and improves noticeably over
both the rule-based baseline and the best fixed algorithm, while adding
only modest overhead. The report thus provides an end-to-end case study
of per-instance algorithm selection for a classical combinatorial
problem.
\end{abstract}

\newpage

\tableofcontents

\newpage
\section{Introduction}

The Traveling Salesman Problem (TSP) is one of the canonical problems in
combinatorial optimisation. Given a set of cities and pairwise travel
costs, the goal is to find the cheapest tour that visits each city once
and returns to the start. The decision version of TSP is NP-complete,
and the number of possible tours grows factorially with the number of
cities. For even moderately sized instances, exhaustive search is
impossible, and one must rely on a rich toolbox of exact methods,
heuristics, and metaheuristics.

Over decades of work this toolbox has become remarkably diverse. Exact
methods based on dynamic programming, branch-and-bound, and cutting
planes can prove optimality on small and medium instances. Constructive
heuristics such as nearest neighbour and insertion rules provide fast
but approximate solutions. Local search and metaheuristics, including
2-opt and 3-opt, simulated annealing, genetic algorithms, ant colony
optimisation, and Lin–Kernighan-style heuristics, typically offer the
best compromise between quality and runtime on larger problems.
However, their performance is strikingly instance-dependent: the same
algorithm that performs well on small Euclidean instances may time out
or give poor solutions on large asymmetric or non-metric ones.

This project began with a simple experiment. I generated synthetic TSP
instances of different sizes and geometric structures, ran several
off-the-shelf solvers on each, and plotted their empirical
quality–runtime trade-offs. The resulting Pareto fronts changed
dramatically with both city count and instance family: exact solvers
dominated on tiny graphs, Lin–Kernighan was best on medium-size
geometric instances, and simple metaheuristics were often the only
methods finishing within the budget on very large or irregular
problems. No single algorithm was consistently near the front, even
though all instances were “just TSPs”.

From a theoretical standpoint this behaviour is consistent with
no-free-lunch theorems for optimisation, which state that no algorithm
can be uniformly best over all possible problems. In practice, it
suggests a different perspective: instead of searching for a single
best TSP algorithm, we should choose an algorithm that is well matched
to the particular instance at hand. This is the idea behind
\emph{algorithm selection} and, in a broader machine learning context,
\emph{AutoML}. In this report I study a focused version of this idea:
given a portfolio of TSP algorithms with diverse strengths and
weaknesses, can we automatically select a good solver for each incoming
instance, using only cheap instance features and within a fixed time
budget?

The system developed here, called \emph{AutoTSP}, treats algorithm
selection as a meta-optimisation problem on top of the TSP. At
prediction time AutoTSP takes a TSP instance and a time budget, computes
a small feature vector describing the instance (such as the number of
cities, metric structure, and simple geometric dispersion measures),
chooses a single algorithm from the portfolio based on these features,
and runs it once within the remaining budget. Conceptually, the user
interacts with AutoTSP as a black-box “meta-solver”: they supply a TSP
instance, and the system handles feature extraction, algorithm choice,
and solver execution.

The report has three main components. First, it builds a heterogeneous
benchmark of TSP and TSP-like instances by combining standard repositories
with synthetic geometric families, and it evaluates a portfolio of exact
methods, constructive heuristics, local search, and metaheuristics on
this benchmark under a strict per-instance time budget. Performance is
measured by a penalised cost that combines tour quality and runtime and
assigns heavy penalties to timeouts and failures, allowing a fair
comparison between algorithms that solve very different subsets of
instances.

Second, it analyses the resulting data from a multi-objective
perspective. For different instance families and size ranges, the report
traces empirical Pareto fronts and studies how they move as instance
features change. This analysis confirms that no single TSP algorithm
dominates across the benchmark: different methods occupy different
regions of the quality–runtime trade-off, and simple instance features
(such as size, metric flags, and nearest-neighbour probes) already give
useful information about which algorithms are likely to work well.

Third, it introduces AutoTSP as a concrete algorithm selection system.
Two selectors are considered: a hand-crafted rule-based selector that
encodes simple heuristics over problem size and structure, and a
learning-based selector implemented as a Random Forest classifier
trained on the benchmark logs. Both selectors are evaluated on a held-out
test set and compared to an oracle that always picks the best algorithm
in hindsight and to a “single best” baseline that always runs the same
solver. The Random Forest selector matches the oracle choice on most
test instances, improves over the best fixed algorithm in terms of
normalised cost, and substantially outperforms the rule-based baseline,
all while adding only modest overhead.

The rest of the report is organised as follows.
\Cref{sec:background} reviews the TSP, relevant variants, and
background on multi-objective optimisation and algorithm selection.
\Cref{sec:algorithms} summarises the algorithm portfolio used in this
work. \Cref{sec:theory} formalises the AutoTSP setting, states the
research questions and hypotheses, and frames algorithm selection as an
optimisation problem. \Cref{sec:methodology} details the experimental
setup, including the instance collection, feature extraction, evaluation
metrics, and benchmarking protocol.
\Cref{sec:empirical} presents an empirical analysis of portfolio
performance and Pareto fronts across instance families.
\Cref{sec:autotp} introduces the AutoTSP selectors and evaluates them
against the oracle and baseline policies.
Finally, \Cref{sec:conclusion} summarises the findings and outlines
directions for future work.











\newpage
\section{Background and Preliminaries}
\label{sec:background}

\subsection{The Traveling Salesman Problem}

The Traveling Salesman Problem (TSP) is a classical combinatorial optimisation problem.  
Given a set of locations and the cost of travelling between every pair of them, the goal is to find the cheapest possible tour that visits each location exactly once and returns to the starting point.

A standard formalisation uses a complete weighted graph. Let
\[
G = (V, E)
\]
be a complete graph with vertex set \(V = \{1, 2, \dots, n\}\) and edge set
\(
E = \{(i,j) : i,j \in V,\, i \neq j\}.
\)
Each edge \((i,j)\) has an associated non-negative cost \(c_{ij} \geq 0\), which might represent distance, time, or some other travel cost.

\begin{definition}[Traveling Salesman Problem]
Given a complete weighted graph \(G = (V, E)\) with edge costs \(c_{ij} \geq 0\), the Traveling Salesman Problem is to find a permutation \(\pi\) of \(\{1,\dots,n\}\) that minimises the total tour cost
\[
    C(\pi) 
    = 
    c_{\pi(n)\,\pi(1)}
    + 
    \sum_{k=1}^{n-1} c_{\pi(k)\,\pi(k+1)} ,
\]
that is, the cost of visiting the vertices in the order \(\pi(1), \pi(2), \dots, \pi(n)\) and then returning from \(\pi(n)\) to \(\pi(1)\).
\end{definition}

This is an optimisation version of the problem; there is also a decision version that asks whether there exists a tour with cost at most a given bound \(B\). The decision version of TSP is well known to be NP-complete, and the optimisation version is NP-hard. In practice this means that, in the worst case, we cannot expect to solve large instances exactly using algorithms whose running time grows only polynomially with \(n\), unless P = NP.

Despite this worst-case hardness, many TSP instances that arise in applications have additional structure. For example, the vertices may correspond to points in the Euclidean plane, and the costs may satisfy symmetry and the triangle inequality. Such structure often makes instances easier to solve in practice and is central to understanding which algorithms work well on which kinds of TSP instances.

\subsection{Variants and Practical Applications}

There are many variants of the TSP, and the choice of variant depends on the underlying application and constraints.

\paragraph{Symmetric vs asymmetric TSP.}
In the \emph{symmetric} TSP (STSP), the cost of travelling from \(i\) to \(j\) is the same as from \(j\) to \(i\), so \(c_{ij} = c_{ji}\) for all \(i,j\). This is a natural model when travel costs are distances along undirected roads or cables.

In the \emph{asymmetric} TSP (ATSP), edge costs may differ in each direction, so in general \(c_{ij} \neq c_{ji}\). This appears in applications such as one-way street networks, different uphill vs downhill travel times, or situations where the cost depends on direction (e.g.\ prevailing winds, currents, or asymmetric processing times).

\paragraph{Metric and Euclidean TSP.}
A common and important special case is the \emph{metric} TSP, where costs satisfy the triangle inequality
\[
    c_{ij} \leq c_{ik} + c_{kj} \quad \text{for all } i,j,k \in V.
\]
Metric TSP models situations where detours cannot be cheaper than direct travel, and many approximation algorithms rely on this property.

The \emph{Euclidean} TSP is a specific metric case where each vertex is a point in \(\mathbb{R}^d\) (often \(d = 2\) or \(3\)), and costs are Euclidean distances between points. Euclidean instances arise naturally in routing and layout problems and are often used as benchmark instances.

\paragraph{Constrained TSP variants.}
In many real-world problems, additional constraints are imposed on tours. Examples include:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item \textbf{Time windows:} each vertex must be visited within a specified time interval.
    \item \textbf{Capacity constraints:} the tour is coupled with delivery or pickup quantities, leading to vehicle routing problems.
    \item \textbf{Prize-collecting or orienteering variants:} not all vertices must be visited; instead, the goal is to collect as much reward as possible within a budget.
\end{itemize}
These extensions are often more realistic but also more challenging to solve.

\paragraph{Practical applications.}
The TSP and its variants appear in a wide range of applications, including:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item \textbf{Logistics and transportation:} planning delivery routes for vehicles, visiting customers or depots efficiently.
    \item \textbf{Manufacturing:} optimising the order of drilling holes on printed circuit boards or machining operations to reduce tool travel.
    \item \textbf{Robotics and inspection:} planning paths for drones, robots, or inspection devices that must visit a set of locations.
    \item \textbf{Data analysis and layout:} ordering tasks or objects to minimise transition costs, or arranging items in a linear order that preserves spatial or similarity relationships.
\end{itemize}

In this report, the primary focus is on \emph{metric} TSP instances, including both Euclidean (coordinate-based) problems and symmetric distance-matrix instances derived from real-world benchmarks. Asymmetric and strongly non-metric cases are included mainly as a secondary robustness testbed and are analysed separately where relevant. This choice keeps the setting concrete while still being rich enough to exhibit diverse algorithmic behaviour across different instance families.


\subsection{Non-Linear and Combinatorial Optimisation Viewpoint}

At a high level, an optimisation problem can be written as
\[
    \min_{x \in \mathcal{X}} f(x),
\]
where \(f : \mathcal{X} \to \mathbb{R}\) is an objective function and \(\mathcal{X}\) is a set of feasible solutions.  
In continuous optimisation, \(\mathcal{X}\) is typically a subset of \(\mathbb{R}^n\), and much of the theory is built around convexity and differentiability.

The TSP fits naturally into the framework of \emph{combinatorial optimisation}. Here the feasible set \(\mathcal{X}\) is discrete: each feasible solution is a tour, which can be represented as a permutation of the vertex set or as a \(0\text{--}1\) incidence vector on the edges. The number of feasible tours grows factorially with the number of vertices, and there is no useful notion of a gradient on this space.

One convenient mathematical encoding introduces binary decision variables \(x_{ij} \in \{0,1\}\), where \(x_{ij} = 1\) means that edge \((i,j)\) is used in the tour and \(x_{ij} = 0\) otherwise. The objective can then be written as a linear function
\[
    \min_{x} \sum_{(i,j) \in E} c_{ij} x_{ij}
\]
subject to a collection of combinatorial constraints that enforce the tour structure (degree constraints and subtour elimination constraints). The objective is linear, but the feasible set defined by these constraints and integrality requirements is highly non-convex. If integrality is relaxed to \(x_{ij} \in [0,1]\), the problem becomes a linear program, but its solutions are in general fractional and do not directly correspond to valid tours.

From the viewpoint of non-linear optimisation, the TSP is therefore a problem with a non-convex feasible region and many local optima when viewed in the natural discrete search space. This is one of the reasons why standard gradient-based methods are not directly applicable, and why specialised exact algorithms and heuristics have been developed for TSP and related combinatorial problems.


\subsection{Multi-Objective Optimisation and Pareto Efficiency}

In this report, algorithms are not judged only by solution quality, but also by their computational cost. This leads to a \emph{multi-objective} viewpoint: instead of a single objective function, we consider several objectives simultaneously.

A general multi-objective optimisation problem can be written as
\[
    \min_{x \in \mathcal{X}} F(x)
    \quad \text{with} \quad
    F(x) = \big(f_1(x), f_2(x), \dots, f_m(x)\big),
\]
where each \(f_k\) measures a different criterion. In our setting, two natural objectives are
\begin{itemize}[noitemsep,topsep=0.5em]
    \item solution quality (for example, tour length or relative optimality gap), and
    \item computational cost (for example, runtime or number of evaluations).
\end{itemize}

Because these objectives often conflict, there is usually no single solution that is best in all respects. Instead, we use the notion of Pareto efficiency to compare solutions.

\begin{definition}[Pareto dominance and Pareto optimality]
Let \(F(x) = (f_1(x), \dots, f_m(x))\) be a vector of objectives to be minimised.  
A solution \(x\) is said to \emph{Pareto-dominate} a solution \(y\) if
\[
    f_k(x) \leq f_k(y) \quad \text{for all } k
    \quad \text{and} \quad
    f_k(x) < f_k(y) \text{ for at least one } k.
\]
A solution \(x^\star\) is \emph{Pareto-optimal} if there is no other solution that Pareto-dominates it. The set of all Pareto-optimal solutions is called the \emph{Pareto front}.
\end{definition}

When we compare TSP algorithms on a given class of instances, each algorithm induces a point in the plane whose coordinates are its typical runtime and typical solution quality. The Pareto front then captures the algorithms that achieve the best trade-offs: moving from one point on the front to another improves one objective only at the expense of worsening another. This perspective will be used throughout the report to describe and compare algorithm behaviour.


\subsection{Algorithm Selection and Meta-Optimisation}

The large number of available algorithms for TSP, each with different strengths and weaknesses, naturally leads to the question of \emph{algorithm selection}. Rather than committing to a single algorithm for all instances, we would like to choose an algorithm that is well-suited to the specific instance at hand.

Let \(\mathcal{I}\) denote a set of problem instances and \(\mathcal{A}\) a finite portfolio of algorithms.  
For each instance \(i \in \mathcal{I}\) and algorithm \(a \in \mathcal{A}\), we can measure a performance vector
\[
    P(a, i) = \big( f_{\text{qual}}(a, i), f_{\text{time}}(a, i) \big),
\]
where \(f_{\text{qual}}\) and \(f_{\text{time}}\) represent solution quality and computational cost. An \emph{algorithm selector} is a rule that, given information about an instance \(i\), chooses an algorithm \(S(i) \in \mathcal{A}\) with the goal of achieving good overall performance across instances.

In practice, the selector usually does not see the instance directly, but only a feature vector \(\phi(i) \in \mathbb{R}^d\) that summarises properties such as problem size, sparsity, or geometric structure. The selection rule can then be viewed as a mapping
\[
    S : \mathbb{R}^d \to \mathcal{A}, \qquad S\big(\phi(i)\big) \text{ is the chosen algorithm for instance } i.
\]
The quality of a selector is measured by how well it performs, on average, compared to simple baselines such as always using the same algorithm.

This viewpoint is an example of \emph{meta-optimisation}: instead of optimising directly over tours, we optimise over choices of algorithms or strategies that themselves solve optimisation problems. Designing AutoTSP can be seen as constructing and improving such a selector for TSP instances. The rest of the report will focus on (i) understanding how algorithm performance varies over different TSP instance families, and (ii) using this understanding to build and evaluate practical selection rules.

\paragraph{Relation to AutoML and algorithm portfolios.}
The AutoTSP setting is closely related to ideas from AutoML and algorithm portfolios. AutoML systems typically select and configure entire machine learning pipelines (models, hyperparameters, and preprocessing steps) for a given dataset, often by running many candidate pipelines and comparing validation performance. In contrast, AutoTSP operates on a fixed combinatorial problem (the TSP) with a small, predefined portfolio of solvers. For each TSP instance, the selector makes a single, one-shot choice of algorithm from this portfolio, based on inexpensive instance features. In this sense, AutoTSP can be viewed as a \emph{one-choice portfolio optimisation} problem: given a portfolio of algorithms and a distribution of instances, choose a mapping from instance features to a single algorithm that optimises the overall quality--runtime trade-off.









\newpage
\section{Algorithms for the Traveling Salesman Problem}
\label{sec:algorithms}

\subsection{Taxonomy of TSP Algorithms}
\label{subsec:taxonomy}

There is a wide range of algorithms for solving the TSP, each making different trade-offs between solution quality, runtime, and implementation complexity. A useful high-level classification splits them into two main groups:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item \textbf{Exact methods}, which always return an optimal tour (up to numerical issues), but may take exponential time in the worst case.
    \item \textbf{Heuristic and metaheuristic methods}, which aim to find very good, but not necessarily optimal, tours in reasonable time, especially for large instances.
\end{itemize}

Within these groups, we can distinguish several families:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item \textbf{Dynamic programming methods} that exploit recursive structure.
    \item \textbf{Integer linear programming and branch-and-bound} methods that search the space of tours using relaxations and bounds.
    \item \textbf{Constructive heuristics} that build a tour step by step (e.g.\ nearest neighbour).
    \item \textbf{Local search heuristics} that start from an initial tour and improve it using local modifications (e.g.\ 2-opt, 3-opt).
    \item \textbf{Metaheuristics} such as simulated annealing, genetic algorithms, and ant colony optimisation, which use higher-level search strategies on top of local moves.
\end{itemize}

Table~\ref{tab:taxonomy} gives a simple summary of these categories and their typical use.

\begin{table}[H]
    \centering
    \begin{tabular}{@{}lll@{}}
        \toprule
        Category & Guarantee & Typical use \\ \midrule
        Exact methods & Optimal solution & Small to medium instances, benchmarks \\
        Constructive heuristics & Feasible, quick & Very large instances, initial solutions \\
        Local search & High-quality tours & Medium to large instances \\
        Metaheuristics & Very high-quality tours & Difficult or structured instances \\ \bottomrule
    \end{tabular}
    \caption{High-level taxonomy of TSP algorithm families.}
    \label{tab:taxonomy}
\end{table}

In the rest of this section we briefly review the main exact and heuristic methods that will be used later in the empirical study and in the design of AutoTSP.


\subsection{Exact Methods}
\label{subsec:exact_methods}

Exact methods aim to find an optimal tour and to certify its optimality. They typically have worst-case exponential running time, but can be very effective on small or moderately sized instances, or on instances with exploitable structure.

Two central ideas appear repeatedly:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item \textbf{Dynamic programming}, which uses overlapping subproblems to avoid recomputing similar tours.
    \item \textbf{Integer linear programming and branch-and-bound}, which relax the problem, compute bounds, and systematically explore the space of tours.
\end{itemize}

These methods form the backbone of many state-of-the-art TSP solvers. Even when the main focus is on heuristics, exact algorithms are important for providing reference optimal values and for understanding the limits of tractable problem sizes.

\subsubsection{Dynamic Programming Approaches}

A classical dynamic programming algorithm for the TSP is the Bellman--Held--Karp algorithm. It works on the complete graph \(G = (V,E)\) with cost matrix \(c_{ij}\) and uses a DP value
\[
    D(S, j)
\]
which represents the minimum cost of a path that starts at a fixed depot (say vertex \(1\)), visits all vertices in the subset \(S \subseteq V\) exactly once, and ends at vertex \(j \in S\).

The recursion is
\[
    D(S, j)
    =
    \min_{i \in S \setminus \{j\}} \big\{ D(S \setminus \{j\}, i) + c_{ij} \big\},
\]
with base cases
\[
    D(\{1\}, 1) = 0.
\]
The optimal tour cost is then obtained by
\[
    \min_{j \neq 1} \big\{ D(V, j) + c_{j1} \big\}.
\]

This algorithm runs in time \(\mathcal{O}(n^2 2^n)\) and space \(\mathcal{O}(n 2^n)\). It is therefore only practical for relatively small numbers of vertices, but it is conceptually simple and useful as a benchmark for optimal solutions on small instances.

\subsubsection{Integer Programming and Branch-and-Bound}

Another powerful approach is to formulate the TSP as an integer linear program (ILP). Using binary variables \(x_{ij} \in \{0,1\}\) to indicate whether edge \((i,j)\) is used in the tour, a basic formulation is
\begin{align*}
    \min_{x} \quad & \sum_{(i,j) \in E} c_{ij} x_{ij} \\
    \text{subject to} \quad
    & \sum_{j : (i,j) \in E} x_{ij} = 1 \quad \forall i \in V, \\
    & \sum_{i : (i,j) \in E} x_{ij} = 1 \quad \forall j \in V, \\
    & \sum_{i,j \in S} x_{ij} \leq |S| - 1 \quad \forall S \subset V,\, S \neq \emptyset, \\
    & x_{ij} \in \{0,1\} \quad \forall (i,j) \in E.
\end{align*}
The degree constraints ensure that each vertex has exactly one incoming and one outgoing edge, while the subtour elimination constraints prevent the solution from breaking into multiple smaller cycles.

In practice, it is not possible to include all subtour elimination constraints explicitly, because there are exponentially many of them. Modern solvers instead use \emph{branch-and-cut}:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item Solve a relaxed problem with only a small subset of constraints.
    \item Analyse the fractional solution to detect violated subtour constraints.
    \item Add these violated constraints as cuts and re-solve.
    \item Use branching to enforce integrality when necessary.
\end{itemize}

This combination of linear programming relaxations, cutting planes, and branch-and-bound is extremely effective on many TSP instances. However, the worst-case running time remains exponential, and performance can vary significantly with problem size and structure. This variability is one of the reasons why algorithm selection, and hence AutoTSP, is a meaningful topic.

\subsection{Heuristic and Metaheuristic Methods}
\label{subsec:heuristics}

Heuristic and metaheuristic methods do not guarantee optimal solutions, but they are often the only practical option for large TSP instances. They aim to find good tours quickly, making trade-offs between solution quality, runtime, and robustness.

Broadly, we can distinguish three families:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item \textbf{Constructive heuristics}, which build a tour step by step.
    \item \textbf{Local search heuristics}, which start from an initial tour and improve it by local modifications.
    \item \textbf{Metaheuristics}, which wrap higher-level search strategies around local moves or constructive steps.
\end{itemize}

In this report, these methods play a central role: they form a large part of the algorithm portfolio used in experiments, and their very different performance profiles across instance types motivate the need for algorithm selection.


\subsubsection{Constructive Heuristics}

Constructive heuristics start from an empty tour and repeatedly add vertices or edges until a complete tour is formed. They are usually fast and simple to implement, and they often provide good initial tours for later improvement by local search.

A basic example is the \emph{nearest neighbour} heuristic:
\begin{enumerate}[noitemsep,topsep=0.5em]
    \item Choose a starting vertex.
    \item At each step, move to the nearest unvisited vertex.
    \item Once all vertices have been visited, return to the start.
\end{enumerate}
This procedure runs in time \(\mathcal{O}(n^2)\) with a naive implementation and often produces reasonable, but not high-quality, tours.

Insertion heuristics follow a different pattern. They start from a small initial cycle and then insert remaining vertices one by one at positions that cause the least increase in total tour length. Common variants include:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item \textbf{Cheapest insertion:} insert the vertex and position that minimise the increase in tour length.
    \item \textbf{Farthest insertion:} always insert the vertex farthest from the current tour, to quickly cover outliers.
\end{itemize}
These heuristics are still relatively cheap to run and usually produce better tours than simple nearest neighbour, making them useful both as standalone methods and as starting points for local search.


\subsubsection{Local Search Heuristics}

Local search heuristics start from an initial tour (often produced by a constructive heuristic) and repeatedly apply small modifications that improve the tour. The search space is defined by a \emph{neighbourhood}, which specifies which tours are considered “close” to the current one.

A common family of neighbourhoods is based on edge exchanges:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item \textbf{2-opt:} remove two edges and reconnect the tour in the only other way that keeps it a single cycle. This is equivalent to reversing a segment of the tour.
    \item \textbf{3-opt:} remove three edges and reconnect the three resulting paths in one of several possible ways.
\end{itemize}
A simple 2-opt algorithm repeatedly applies the best improving 2-edge swap until no further improvement is possible. This usually runs in polynomial time per iteration but can require many iterations; in practice, variants that limit the set of candidate swaps are used to keep runtimes manageable.

Local search methods often achieve much shorter tours than pure constructive heuristics, especially on Euclidean instances. However, they can get stuck in local minima, where no small modification leads to an improvement. This motivates metaheuristics that allow the search to escape local minima.


\subsubsection{Metaheuristics}

Metaheuristics provide general frameworks for exploring the search space in a more global way. They typically rely on local moves like 2-opt but add mechanisms to escape local minima, diversify the search, or intensify it around promising regions.

Common examples for the TSP include:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item \textbf{Simulated annealing:} occasionally accepts worse moves according to a temperature parameter that gradually decreases, allowing the search to jump out of local minima early on.
    \item \textbf{Genetic algorithms:} maintain a population of tours and combine them using crossover and mutation operators, guided by selection based on tour length.
    \item \textbf{Ant colony optimisation:} models artificial “ants” that construct tours probabilistically, influenced by pheromone trails that encode good edges discovered in previous iterations.
\end{itemize}

These methods are more complex to tune and implement than basic local search, but they can reach very high-quality tours on difficult instances. They also tend to show strong instance-dependent behaviour, which makes them interesting candidates in an algorithm selection setting.


\subsection{Qualitative Comparison}
\label{subsec:qualitative_comparison}

Exact methods, constructive heuristics, local search, and metaheuristics each have distinct strengths and weaknesses. Table~\ref{tab:qualitative} summarises some typical characteristics.

\begin{table}[H]
    \centering
    \begin{tabular}{@{}llll@{}}
        \toprule
        Family & Runtime (typical) & Solution quality & Notes \\ \midrule
        Exact (DP, ILP) 
            & High, exponential & Optimal & Small to medium \(n\), benchmarks \\
        Constructive heuristics 
            & Very low & Moderate & Good initial tours, scalable \\
        Local search (2-opt, 3-opt) 
            & Low to moderate & High & Sensitive to initial tour and neighbourhood \\
        Metaheuristics 
            & Moderate to high & Very high & Many parameters, instance-dependent \\ \bottomrule
    \end{tabular}
    \caption{Qualitative comparison of TSP algorithm families.}
    \label{tab:qualitative}
\end{table}

These qualitative differences explain why no single method dominates across all instance types and sizes. In later chapters, we will quantify these trade-offs using empirical Pareto curves and use the resulting patterns to design the AutoTSP selection rules.












\newpage
\section{Theoretical Motivation for AutoTSP}
\label{sec:theory}

\subsection{No Free Lunch Theorems for Optimisation}
\label{subsec:nfl}

The no free lunch (NFL) theorems are a family of results that formalise the idea that, when averaged over all possible optimisation problems, no algorithm is better than any other. In their simplest form, they consider a finite search space \(\mathcal{X}\), a finite set of objective values \(\mathcal{Y}\), and the set \(\mathcal{F} = \{ f : \mathcal{X} \to \mathcal{Y} \}\) of all possible objective functions.

An optimisation algorithm can be seen as a procedure that, at each step, chooses the next point \(x \in \mathcal{X}\) to evaluate based on the history of previously queried points and their values. Its performance on a function \(f\) is measured by some summary of the observed values \(f(x_1), f(x_2), \dots\) after a fixed number of evaluations.

Very roughly, an NFL theorem states that if all functions in \(\mathcal{F}\) are equally likely, then the average performance of any two algorithms is the same. In other words, for a uniform prior over \(\mathcal{F}\),
\[
    \mathbb{E}_{f \in \mathcal{F}}\big[ \text{Perf}(A_1, f) \big]
    =
    \mathbb{E}_{f \in \mathcal{F}}\big[ \text{Perf}(A_2, f) \big]
\]
for any two algorithms \(A_1\) and \(A_2\), where \(\text{Perf}\) is a reasonable performance measure.

This has an important but limited message:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item If we know nothing about the structure of the objective functions we face, then no algorithm can be universally better than another.
    \item In practice, real problems occupy a tiny, structured subset of \(\mathcal{F}\), so some algorithms can perform much better than others on the problems we actually care about.
\end{itemize}

For the TSP, this suggests that we should not expect a single algorithm to dominate on all instance types. Instead, different algorithms will tend to work well on different families of instances (for example, small versus large, Euclidean versus non-Euclidean, dense versus sparse). This observation motivates the study of \emph{algorithm selection}: rather than searching for one best algorithm, we try to choose an algorithm that is well matched to each instance.


\subsection{Algorithm Selection as an Optimisation Problem}
\label{subsec:alg_sel_problem}

We now formalise the algorithm selection problem that underlies AutoTSP. Let \(\mathcal{I}\) be a set of problem instances (e.g.\ TSP instances), and let \(\mathcal{A} = \{a_1, \dots, a_K\}\) be a finite portfolio of algorithms. For each pair \((a, i) \in \mathcal{A} \times \mathcal{I}\), we can measure a performance vector
\[
    P(a, i) = \big( f_{\text{qual}}(a,i),\, f_{\text{time}}(a,i) \big),
\]
where \(f_{\text{qual}}\) measures solution quality (e.g.\ tour length or optimality gap) and \(f_{\text{time}}\) measures computational cost (e.g.\ runtime).

An \emph{algorithm selector} is a mapping that, given some information about an instance \(i\), chooses an algorithm from \(\mathcal{A}\). In practice, the selector does not see the full instance directly, but only a feature vector \(\phi(i) \in \mathbb{R}^d\) that encodes properties such as:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item number of vertices,
    \item density or sparsity,
    \item geometric statistics (for Euclidean TSP),
    \item presence of particular constraints.
\end{itemize}
We can thus model a selector as a function
\[
    S : \mathbb{R}^d \to \mathcal{A}, \qquad S\big(\phi(i)\big) \text{ is the chosen algorithm for instance } i.
\]

To evaluate a selector, we assume a distribution \(\mathcal{D}\) over instances (or over instance families) and define an aggregate performance measure. For example, if we focus on a scalarised objective
\[
    g(a,i) = \alpha\, f_{\text{qual}}(a,i) + (1-\alpha)\, f_{\text{time}}(a,i),
\]
with a trade-off parameter \(\alpha \in [0,1]\), then the expected performance of selector \(S\) is
\[
    \mathcal{L}(S)
    =
    \mathbb{E}_{i \sim \mathcal{D}} \big[ g\big(S(\phi(i)), i\big) \big].
\]
The algorithm selection problem is to find a selector \(S^\star\) that minimises \(\mathcal{L}(S)\) over some class of selectors (for example, all rule-based selectors of a certain form, or all models within a given machine learning family).

In this sense, designing AutoTSP is itself an optimisation problem defined over the space of selectors. The hand-crafted rule-based selector and the random forest selector used in this report are two concrete choices for \(S\). In later chapters we will compare their performance to simple baselines and to an oracle that always chooses the best algorithm for each instance.


\subsection{Multi-Objective Trade-offs: Quality vs Runtime}
\label{subsec:multiobj_tradeoff}

In practice, TSP algorithms are judged on at least two criteria:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item \textbf{Solution quality}, for example tour length or relative optimality gap.
    \item \textbf{Computational cost}, for example runtime or number of evaluations.
\end{itemize}
Improving one criterion often worsens the other. An algorithm that explores the search space more thoroughly may find shorter tours, but will usually take longer to run.

Formally, for each algorithm \(a \in \mathcal{A}\) and instance \(i \in \mathcal{I}\) we have a performance vector
\[
    P(a,i) = \big(f_{\text{qual}}(a,i), f_{\text{time}}(a,i)\big),
\]
with both components to be minimised. When comparing algorithms or selectors, it is therefore not enough to look at a single scalar summary; we should consider the trade-off between quality and runtime.

One way to do this is to impose a \emph{time budget}: fix a maximum allowed runtime \(T\), and compare algorithms only by the quality they achieve within time \(T\). Another is to use a \emph{scalarisation}, combining the objectives into a single score, for example
\[
    g(a,i) = \alpha\, f_{\text{qual}}(a,i) + (1-\alpha)\, f_{\text{time}}(a,i),
\]
for some \(\alpha \in [0,1]\). Both views are useful in experiments.

At a more structural level, we can consider the set of achievable performance vectors and study its \emph{Pareto front}: the set of algorithms (or selectors) for which no other choice is strictly better in both quality and runtime. This multi-objective viewpoint fits naturally with the AutoTSP goal of choosing algorithms that lie close to the Pareto front for each instance or instance family.


\subsection{Problem Formulation for AutoTSP}
\label{subsec:autotsp_formulation}

We now specialise the general algorithm selection setting to the TSP and define the AutoTSP problem. Recall the portfolio of algorithms \(\mathcal{A} = \{a_1,\dots,a_K\}\) and the set of TSP instances \(\mathcal{I}\). Each instance \(i \in \mathcal{I}\) is mapped to a feature vector \(\phi(i) \in \mathbb{R}^d\). A selector
\[
    S : \mathbb{R}^d \to \mathcal{A}
\]
chooses an algorithm \(S(\phi(i))\) based on these features.

For a given selector \(S\), the performance on instance \(i\) is captured by
\[
    P(S,i) = \big(f_{\text{qual}}(S(\phi(i)), i),\, f_{\text{time}}(S(\phi(i)), i) + f_{\text{sel}}^S(i)\big),
\]
where \(f_{\text{sel}}^S(i)\) denotes the computational cost of computing features and making the selection decision. In most practical designs, \(f_{\text{sel}}^S(i)\) should be small compared to the cost of running the chosen algorithm.

To define an optimisation goal, we assume a distribution \(\mathcal{D}\) over instances (or a representative finite set) and either:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item fix a time budget \(T\) and minimise expected solution quality subject to \(f_{\text{time}} + f_{\text{sel}}^S \leq T\), or
    \item use a scalarised objective
    \(
        g(S,i) = \alpha\, f_{\text{qual}}(S(\phi(i)), i) + (1-\alpha)\, f_{\text{time}}(S(\phi(i)), i)
    \)
    and minimise the expected value
    \(
        \mathbb{E}_{i \sim \mathcal{D}}[g(S,i)].
    \)
\end{itemize}

The \emph{AutoTSP problem} is then:
\begin{quote}
    Given a portfolio \(\mathcal{A}\), an instance feature mapping \(\phi\), and an evaluation scheme as above, find a selector \(S\) within a chosen class (e.g.\ rule-based selectors or random forest models) that achieves the best trade-off between solution quality and runtime over instances drawn from \(\mathcal{D}\).
\end{quote}

In this report we study two concrete classes of selectors:
\begin{enumerate}[noitemsep,topsep=0.5em]
    \item a manually designed rule-based selector, and
    \item a learning-based selector implemented as a random forest.
\end{enumerate}
Their performance will be compared against simple baselines (such as always using a single fixed algorithm) and against an oracle that always picks the best algorithm for each instance.

From this viewpoint, the AutoTSP selector does not schedule or combine algorithms; it performs a single portfolio decision: for each instance, pick exactly one algorithm from the portfolio to run. This one-choice portfolio setting is simpler than dynamic portfolio approaches, but fits many practical scenarios and keeps selection overhead small.



\subsection{Research Questions and Hypotheses}
\label{subsec:rq_hypotheses}

The AutoTSP problem, as formulated above, leads to a small set of concrete questions. These guide both the design of experiments and the analysis of results.

\paragraph{RQ1: Instance-dependent behaviour of TSP algorithms.}
\emph{How does the performance of individual TSP algorithms vary across different instance families and instance features?}

This question is about understanding the landscape before attempting any selection. The hypothesis is that:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item[\textbf{H1}] No single algorithm in the portfolio is Pareto-dominant across all instance families; instead, different algorithms occupy different parts of the quality--runtime trade-off depending on problem size and structure.
\end{itemize}

\paragraph{RQ2: Value of per-instance algorithm selection.}
\emph{Can a simple per-instance selector outperform the best single fixed algorithm, on average, under realistic time budgets?}

Here we compare AutoTSP against baselines such as always using the same algorithm. The main hypothesis is:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item[\textbf{H2}] A rule-based selector that chooses algorithms based on a few simple features (such as number of vertices and basic structural statistics) achieves lower average regret to the oracle selector than any single fixed algorithm in the portfolio.
\end{itemize}
As discussed in \Cref{subsec:selector-eval}, this hypothesis is \emph{not} supported by the empirical results: in practice the rule-based policy is dominated by both the single best algorithm and the learning-based selector.

\paragraph{RQ3: Benefit of learning-based selection over manual rules.}
\emph{Does a learning-based selector (random forest) provide a measurable improvement over a manually designed rule-based selector, once selection overhead is taken into account?}

This question distinguishes between hand-crafted and data-driven selection. The hypothesis is:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item[\textbf{H3}] A random forest selector trained on instance features and observed algorithm performance achieves lower average regret than the manual rule-based selector, while incurring negligible additional selection cost.
\end{itemize}

These research questions structure the rest of the report.  
Section~\ref{sec:empirical} addresses \textbf{RQ1} by empirically characterising algorithm performance across instance families.  
Section~\ref{sec:autotp} addresses \textbf{RQ2} and \textbf{RQ3} by constructing the AutoTSP selectors and comparing them to baselines and to an oracle selector.

















\newpage
\section{Experimental Methodology}
\label{sec:methodology}

\subsection{Overview of the AutoTSP Pipeline}

AutoTSP takes a TSP instance as input and, within a fixed time budget,
chooses and runs a single algorithm from the portfolio on that instance.
The pipeline is designed so that all decisions are made from cheap,
instance–level information rather than from trial runs of the solvers
themselves.

At prediction time the pipeline proceeds in four stages.
\begin{enumerate}[noitemsep,topsep=0.5em]
  \item \textbf{Instance normalisation.}
    The incoming instance is represented as a complete weighted graph,
    together with basic attributes such as the number of cities, metric
    type, symmetry, and origin. This normalised view hides the details
    of how the instance was originally specified (coordinates versus
    distance matrix, symmetric versus asymmetric file formats).
  \item \textbf{Feature extraction.}
    A lightweight feature extractor computes a small set of numerical
    and Boolean descriptors of the instance: problem size, indicators of
    metric structure, measures of spatial spread, and simple probes of
    typical edge lengths and tour scales. The extractor also measures
    how long it took to compute these features, so that this overhead
    can be accounted for in the remaining budget.
  \item \textbf{Algorithm selection.}
    The selector receives the feature vector and the remaining time
    budget and returns a single algorithm from the portfolio. In the
    rule–based version this choice is determined by hand–crafted
    thresholds on features such as the number of cities and metric type.
    In the learning–based version a Random Forest model, trained on
    historical benchmark data, predicts the algorithm that is expected
    to perform best under the budget.
  \item \textbf{Solver execution and reporting.}
    The chosen solver is run once on the instance, with a strict
    wall–clock limit equal to the remaining budget. It produces a tour
    (if found), its cost, and the actual runtime and status. AutoTSP
    then returns this outcome together with metadata describing which
    algorithm was selected, which features were used, and how the time
    budget was spent between feature extraction and solving.
\end{enumerate}

This architecture ensures that AutoTSP can be used as a black–box
“meta–solver”: users provide a TSP instance and an overall time budget,
and the system handles feature computation, algorithm choice, and solver
execution in a single call.

\subsection{Algorithm Portfolio}

AutoTSP operates on a fixed but heterogeneous portfolio of TSP
algorithms. The portfolio is deliberately constructed to span very
different parts of the quality–time trade–off, so that the selector has
meaningful choices to make. Broadly, it contains exact algorithms,
approximation schemes, constructive and local–search heuristics, and
metaheuristics.

\paragraph{Exact methods.}
The exact solvers include classic dynamic–programming and
branch–and–bound approaches (such as Held–Karp and a generic
branch–and–bound tree search), cutting–plane methods, and a
Concorde–style solver for symmetric Euclidean instances. These methods
guarantee optimal tours but have exponential worst–case complexity and
in practice are only viable on small to moderate instances. They are
included in the portfolio to provide a high–quality baseline on small
instances and to anchor the empirical Pareto fronts.

\paragraph{Approximation algorithm.}
To bridge the gap between exact methods and heuristics, the portfolio
includes Christofides’ algorithm for metric TSP. This constructive
approximation scheme builds a tour by combining a minimum spanning tree
with a minimum–weight matching on odd–degree vertices. It runs in
polynomial time and enjoys a worst–case $1.5$–approximation guarantee
on symmetric metric instances, making it an attractive option when exact
methods are already too slow but strong guarantees are still desired.

\paragraph{Constructive and local–search heuristics.}
Several pure heuristics provide fast, scalable baselines: simple
nearest–neighbour construction, a multi–start nearest–neighbour variant
that restarts from different initial cities, problem–specific
constructive heuristics that exploit spatial structure, and local–search
methods based on 2–opt and 3–opt edge exchanges. These algorithms are
generally very cheap to run and scale to large $n$, but the tour
quality they achieve depends strongly on the instance geometry and the
initial tour. In the portfolio they cover the “fast but approximate”
corner of the Pareto frontier.

\paragraph{Metaheuristics.}
Finally, the portfolio contains several metaheuristic algorithms that
wrap higher–level search strategies around local moves. These include
simulated annealing, a genetic algorithm, iterated local search, ant
colony optimisation, and a Lin–Kernighan–style heuristic (LKH) for
larger symmetric instances. Metaheuristics typically incur higher
computational cost per run than simple local search but can reach much
shorter tours on difficult instances by escaping poor local minima and
exploring the search space more aggressively.

From an implementation perspective, most of these algorithms are
implemented in pure Python on top of basic numerical routines. Two
notable exceptions are the Concorde–based exact solver, which calls the
highly optimised C implementation via a Python interface, and the
Lin–Kernighan–style heuristic, which invokes the LKH code. Their
measured runtimes therefore reflect both algorithmic efficiency and the
advantages of compiled code. The simpler Python implementations of
branch–and–bound, dynamic programming, and genetic and local–search
heuristics should be seen as reference baselines rather than
state–of–the–art engineering efforts.

Taken together, this portfolio exposes a rich variety of performance
profiles across instance types and sizes. Some algorithms are extremely
strong on small or highly structured problems but fail or time out on
larger ones; others are robust but only moderately accurate. The central
task of AutoTSP is to exploit these differences by choosing, for each
incoming instance, the algorithm whose expected quality–time trade–off
is most favourable under the given budget.

\subsection{TSP Instance Families}

The empirical study is based on a mixture of synthetic and external TSP
instances, all mapped into the common weighted–graph representation
described in \Cref{subsec:implementation-details}. The synthetic
instances are designed to span a range of geometric structures, while
the external instances provide realistic benchmarks drawn from standard
collections.

\paragraph{Synthetic Euclidean and Manhattan families.}
For symmetric geometric TSPs we generate instances by sampling points in
the plane and defining edge weights via Euclidean distances. Several
families are considered:
\begin{itemize}[noitemsep,topsep=0.5em]
  \item \emph{Uniform Euclidean clouds}, where cities are independently
    and uniformly distributed over a square region, yielding relatively
    homogeneous distance structure.
  \item \emph{Clustered, ring–cluster, and corridor instances}, where
    cities form tight clusters, are arranged along a ring, or lie in a
    narrow corridor, creating bottlenecks and highly anisotropic
    geometries.
  \item \emph{Grid–like and rotated grids}, where points lie near a
    perturbed lattice, with rotated variants to break axis–alignment
    assumptions.
  \item \emph{Noisy Euclidean variants}, where small perturbations are
    added to otherwise regular patterns to test robustness to local
    irregularities.
\end{itemize}
In addition, we generate Manhattan instances by sampling points
uniformly in the plane and measuring distances in the $\ell_1$ norm,
both in a clean form and with additive noise. These families provide
metric but non–Euclidean behaviour and probe how strongly solvers rely
on Euclidean structure.

\paragraph{Asymmetric and matrix–based instances.}
To cover asymmetric travelling–salesman problems (ATSP) and situations
where only a distance matrix is available, we include synthetic
instances with explicitly asymmetric cost matrices. Some are derived
from Euclidean distances with directional skew, while others are drawn
as random non–symmetric matrices subject to basic consistency
conditions. These instances separate the effects of asymmetry from
those of geometric layout.

\paragraph{External benchmark instances.}
External problems are ingested from standard TSP and ATSP collections
and from numeric matrix files. They include both coordinate–based
instances (such as classic TSPLIB benchmarks) and pure distance
matrices arising from application–specific models. All such instances
are parsed into the unified graph representation, annotated with their
origin and basic structural attributes, and then deduplicated by
comparing content–based hashes of their distance data and metadata. This
prevents overlapping datasets from contributing essentially identical
instances to the benchmark.

\paragraph{Size ranges and allocation.}
Across all families we consider a wide range of problem sizes. For
synthetic geometric instances we sample city counts on a logarithmic
scale, starting from very small problems with around five cities and
extending up to instances with roughly one to two thousand cities.
Within each combination of family and size, we generate multiple
independent instances from a fixed random seed to capture variability.
External instances span a similar size range, with an optional cap on
the maximum number of cities to keep solver runtimes manageable.

The resulting dataset contains, for each family, a modest number of
instances per size (rather than an exhaustive enumeration), but covers
many distinct structural patterns and scales. This balance between
diversity and per–family sample size is chosen to stress the
instance–dependence of algorithm performance and to provide meaningful
training and test distributions for the AutoTSP selectors.

\subsection{Dataset: TSP instance collection}
\label{subsec:dataset}

The AutoTSP experiments require a heterogeneous collection of TSP-type instances that spans a range of sizes, structures, and constraint types. In line with the problem setting in \Cref{sec:background}, the primary target is metric TSP: Euclidean coordinate-based problems and symmetric distance matrices that approximately satisfy the triangle inequality. The combined benchmark also contains asymmetric and more irregular matrix-explicit instances, which are retained mainly as a secondary robustness testbed. To assemble this collection, instances were gathered from several established public repositories, covering both symmetric and asymmetric TSP, as well as simple extensions such as time windows. All instances are converted to a unified JSONL format using a common ingestion pipeline.

\subsubsection*{Sources}

The instance collection includes data from the following sources:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item \textbf{TSPLIB95}: classical benchmark instances for symmetric and asymmetric TSP, provided in TSPLIB format and widely used for comparative studies.\cite{reinelt1991tsplib,tsplib95_website}
    \item \textbf{FamilyTSP generated instances}: instances generated from symmetric TSPLIB problems with additional family structure, enriching the variety of Euclidean-like graphs.\cite{familytsp_instances}
    \item \textbf{Cirasella asymmetric instances}: a set of asymmetric TSP instances created for algorithmic studies and challenge benchmarks, based on several ATSP instance generators.\cite{cirasella2001atsp,cirasella_instances_site}
    \item \textbf{TSP with time windows (TSPTW)}: instances with time-window constraints, providing TSP-like problems with additional temporal structure.\cite{lopezibanez_tsptw_instances,lopezibanez2009beamaco}
    \item \textbf{\texttt{mastqe/tsplib} mirror}: a GitHub mirror of TSPLIB-style instances, used as a convenient scripted source; overlapping instances are deduplicated at ingestion time.\cite{mastqe_tsplib}
    \item \textbf{DIMACS TSP/ATSP collections}: instances used in the DIMACS TSP challenge, including directed problems and explicit distance matrices.\cite{dimacs_tsp_challenge}
    \item \textbf{Waterloo TSP data sets}: larger Euclidean TSP instances based on geographic coordinates, including national and world-scale examples.\cite{uwaterloo_tsp_test_data}
\end{itemize}

Together, these sources yield symmetric and asymmetric instances with a wide range of city counts, metric and non-metric cost structures, and the presence or absence of additional constraints (such as time windows).

\subsubsection*{Ingestion and normalisation}

All instances are converted to a common internal representation by a Python ingestion script. The script recursively scans a specified root directory for files with extensions
\[
\{\texttt{.tsp}, \texttt{.atsp}, \texttt{.txt}, \texttt{.dat}, \texttt{.tw}\},
\]
and attempts to parse each file as follows:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item Files with extensions \texttt{.tsp} or \texttt{.atsp} are parsed using a dedicated \texttt{parse\_tsplib} function that handles TSPLIB-style headers, coordinate sections, and explicit distance matrices.
    \item Other text files are first passed to \texttt{parse\_numeric\_matrix}, which attempts to interpret the contents as a full distance matrix without headers. If this fails, \texttt{parse\_header\_matrix} is used, which expects a leading dimension \(n\) followed by an \(n \times n\) matrix.
\end{itemize}

For each successfully parsed file, a base record is constructed containing, where available:
\begin{itemize}[noitemsep,topsep=0.5em]
    \item node \texttt{coordinates} or a full \texttt{distance\_matrix},
    \item the inferred \texttt{metric} and \texttt{problem\_type},
    \item the \texttt{origin} and \texttt{source\_name} (dataset and file stem),
    \item a Boolean \texttt{directed} flag,
    \item \texttt{num\_cities}, derived from the coordinate array or matrix dimension.
\end{itemize}

A content-based identifier \texttt{problem\_id} is then computed for each record using a helper function \texttt{compute\_problem\_id}. Before a record is written, this identifier is checked against those already present in the output JSONL file. If the identifier has been seen, the instance is skipped. This mechanism removes duplicates arising from overlapping sources (for example, TSPLIB95 and its mirrors).

The script supports an optional \texttt{--max-cities} argument; instances whose number of cities exceeds this limit are discarded. This allows the dataset to be restricted to problem sizes that are practical for the chosen algorithm portfolio.

After parsing, filtering, and deduplication, the script writes each instance as a JSON object to a single \texttt{problems.jsonl} file. This file forms the unified TSP instance collection used throughout the empirical analysis and in the training and evaluation of the AutoTSP selectors.

\subsection{Instance Features}
\label{subsec:instance-features}

To drive the AutoTSP selector we extract a low–dimensional feature vector
$\phi(i) \in \mathbb{R}^d$ for each TSP instance $i$. The goal is not to
encode the full instance in a lossless way, but to capture the coarse
properties that most strongly influence algorithm performance: size,
geometry, and metric structure. Feature extraction is implemented in
\texttt{AutoTSP.features.FeatureExtractor} and is deliberately lightweight so
that selection overhead remains negligible relative to solver runtimes.

Given an instance $i$ with either an explicit distance matrix or a set of
2D coordinates, we compute:

\begin{itemize}
  \item \textbf{Problem size.}
    The number of cities $n$:
    \[
      n(i) = \texttt{n\_nodes} \in \mathbb{N}.
    \]

  \item \textbf{Metric flag.}
    A Boolean indicator \texttt{is\_metric} which is \emph{true} if the
    distance data appear to define a symmetric metric (zero diagonal,
    approximate symmetry, and no obvious triangle inequality violations),
    and \emph{false} otherwise. This distinguishes Euclidean / TSPLIB--EUC\_2D
    instances from arbitrary asymmetric or non–metric distance matrices.

  \item \textbf{Spatial dispersion.}
    For coordinate–based instances we compute the per–dimension standard
    deviations
    \[
      \texttt{std\_dev\_x},\ \texttt{std\_dev\_y}
    \]
    of the city coordinates, as well as the area of the bounding box
    \[
      \texttt{bbox\_area} = (\max_x - \min_x)(\max_y - \min_y).
    \]
    These quantities reflect how spread out the points are and therefore roughly how long typical edges and tours are.

  \item \textbf{Centroid dispersion.}
    Let $c$ be the centroid of the coordinates. We compute the mean
    Euclidean distance from cities to the centroid,
    \[
      \texttt{centroid\_dispersion}
      = \frac{1}{n}\sum_{v} \lVert x_v - c \rVert_2,
    \]
    which distinguishes tightly clustered instances from those with
    widely scattered points.

  \item \textbf{Landmark distance.}
    We select a random ``landmark'' city and compute the average distance
    from the landmark to a small random sample of other cities:
    \[
      \texttt{landmark\_10\_dist}
      \approx \frac{1}{k}\sum_{j=1}^k d(\ell, v_j).
    \]
    This provides a cheap probe of typical pairwise distances even when
    only a distance matrix is available.

  \item \textbf{Nearest--neighbour probe.}
    Finally, we compute
    \texttt{nn\_probe\_cost} by running a truncated nearest–neighbour
    heuristic tour on a small subset of the cities (about $10\%$ of $n$).
    This produces a very fast, coarse estimate of a tour length scale for
    the instance. It is used both as an informative feature for the
    selector and as a building block for our evaluation penalty scheme.
\end{itemize}

For instances that are given only as distance matrices, we compute the
subset of features that can be derived without coordinates (metric flag,
landmark distance, nearest–neighbour probe), and set the remaining
coordinate–based features to \texttt{None}. This keeps feature extraction
robust across both synthetic geometric instances and real--world TSPLIB
and matrix–file instances while maintaining a consistent interface for
the selector.


\subsection{Evaluation Metrics}
\label{subsec:evaluation-metrics}

The goal of the evaluation is to compare different selection policies
(rule–based, Random Forest, simple baselines) in a way that reflects
their performance on a \emph{distribution} of instances, rather than on
hand–picked examples. Importantly, we must account for algorithms that
time out or fail, not just average over successful runs, otherwise exact
methods that only solve tiny instances would appear unfairly strong.

\paragraph{Per–instance penalised cost.}
We fix a per–instance wall–clock budget $T$ (here $T = 10$ seconds). For
each instance $i$ and algorithm $a$ we consider a single run from the
benchmark data, with tour cost $q(a,i)$, runtime $t(a,i)$, and status
(success, timeout, infeasible, etc.). We then define a \emph{penalised
cost}
\[
  \tilde{q}(a,i) =
  \begin{cases}
    q(a,i), & \text{if the run succeeds within } T, \\
    C(i),   & \text{otherwise},
  \end{cases}
\]
where $C(i)$ is an instance–specific penalty that is guaranteed to be
worse than any reasonable tour on that instance.

To obtain $C(i)$ we construct a per–instance upper bound scale
$U(i)$ which depends only on the geometry and size of the instance,
not on how well other algorithms performed:

\begin{itemize}
  \item \textbf{Nearest–neighbour based bound (default).}
    We approximate a full–tour upper bound by scaling the cheap nearest–
    neighbour probe cost. Let $\texttt{nn\_probe\_cost}(i)$ be the cost of
    the truncated nearest–neighbour tour on a subset of cities, and let
    $n(i)$ be the number of cities. If the probe visits $k(i)$ cities,
    we estimate a full tour scale as
    \[
      U_{\text{nn}}(i)
      \approx \texttt{nn\_probe\_cost}(i) \cdot \frac{n(i)}{k(i)}.
    \]

  \item \textbf{Geometric bound.}
    As a complementary option, we use a simple geometric upper bound. For
    coordinate–based instances let $\Delta(i)$ be the diagonal of the
    bounding box of the coordinates, and for matrix–based instances let
    $D_{\max}(i)$ be the maximum entry of the distance matrix. Then any
    tour has length at most $n(i)\Delta(i)$ or $n(i)D_{\max}(i)$,
    respectively, so we can set
    \[
      U_{\text{geom}}(i) = n(i) \cdot \max\{\Delta(i), D_{\max}(i)\}.
    \]
\end{itemize}

In both cases we define the failure penalty as
\[
  C(i) = \alpha \, U(i),
\]
with a fixed factor $\alpha > 1$ (in our experiments $\alpha = 2$).
Thus, a timeout or infeasible run is treated as substantially worse than
any realistic successful tour on the same instance. This design ensures
that algorithms which frequently fail or time out have high average
penalised cost and cannot appear strong simply because they only solve
easy instances.

\paragraph{Oracle and baselines.}
Given $\tilde{q}(a,i)$ for all algorithms $a$ on instance $i$, the
\emph{oracle} selector chooses the algorithm with minimum penalised cost:
\[
  a^\star(i) = \arg\min_{a} \tilde{q}(a,i).
\]
Because costs are to be minimised, this oracle provides a lower bound on the average penalised cost achievable by any selector that chooses among the same algorithm set based only on instance features.

We also define a \emph{single best algorithm} (SBA) baseline. For each
algorithm $a$ we compute its average penalised cost over an evaluation set $I$ of instances:
\[
  \bar{q}(a) = \frac{1}{|I|} \sum_{i \in I} \tilde{q}(a,i),
\]
and choose
\[
  a_{\text{SBA}} = \arg\min_{a} \bar{q}(a).
\]
The SBA policy corresponds to always running $a_{\text{SBA}}$,
independent of instance features. Because we use the penalised cost
$\tilde{q}$, algorithms that often fail or exceed the time budget are
naturally disfavoured.

\paragraph{Selector performance and regret.}
A selector $S$ maps each instance $i$ to an algorithm $a_S(i)$.
We define its penalised cost on instance $i$ as
\[
  \tilde{q}_S(i) = \tilde{q}(a_S(i), i),
\]
and its \emph{regret} relative to the oracle as
\[
  r_S(i) = \tilde{q}_S(i) - \tilde{q}(a^\star(i), i).
\]
We then report summary statistics over the evaluation set $I$:
\begin{itemize}
  \item \textbf{Mean and median regret:}
    \[
      \overline{r}_S =
      \frac{1}{|I|} \sum_{i \in I} r_S(i),
      \qquad
      \text{median}(r_S).
    \]
    Low regret indicates that the selector tends to choose algorithms
    whose performance is close to the oracle.

  \item \textbf{Top–1 accuracy vs oracle:}
    \[
      \text{acc}_S
      = \Pr_{i \sim I}[a_S(i) = a^\star(i)],
    \]
    i.e.\ the fraction of instances on which the selector picks exactly
    the same algorithm as the oracle.

  \item \textbf{Total wall–clock time:}
    For each instance we also track the total time spent on feature
    extraction, selection, and solving,
    \[
      t_S^{\text{tot}}(i)
      = t_{\text{feat}}(i) + t_{\text{select}}^S(i) + t(a_S(i), i),
    \]
    and report the average $\overline{t}_S^{\text{tot}}$ across instances.
\end{itemize}

These metrics allow us to compare the rule–based selector, the
Random Forest selector, the SBA baseline, and the oracle on a common,
failure–aware scale. Because timeouts and infeasible runs are penalised
in a way that depends only on the instance (via $U(i)$), we obtain a
more realistic picture of selector quality than if we averaged only over
successful runs.


\subsection{Experimental Protocol}
\label{subsec:experimental-protocol}

All experiments follow the same end–to–end pipeline:
\emph{(i)} construct a benchmark distribution of TSP instances,
\emph{(ii)} evaluate a fixed portfolio of algorithms on each instance
under per–run resource constraints, \emph{(iii)} derive per–instance
performance summaries and penalised costs as in
\Cref{subsec:evaluation-metrics}, and \emph{(iv)} train and evaluate
algorithm selectors on this benchmark.

\paragraph{Benchmark instance distribution.}
The benchmark instances combine synthetic TSPs and external benchmark
problems. Synthetic instances are generated by sampling city
coordinates in the plane and deriving distances from either Euclidean
or Manhattan metrics. Several geometric families are represented,
including uniform point clouds, perturbed grids, clustered point sets,
rings of clusters, circular instances, and noisy variants, so that the
portfolio encounters both “easy” and highly structured instances.
For each family we generate a grid of problem sizes ranging from very
small problems with $n \approx 5$ cities up to large instances with
thousands of cities, and draw multiple independent realisations per
size from a fixed pseudo–random generator.

External instances are drawn from standard TSP benchmark collections
(such as TSPLIB) and from matrix–based datasets. These problems arrive
in a variety of formats (coordinate files, full distance matrices,
asymmetric cost tables) and are normalised into a common representation
as weighted complete graphs together with high–level metadata:
problem type (symmetric or asymmetric), metric family, and origin.
For coordinate–based problems, optional geometric transformations
such as rotation or rescaling are applied to enrich the variety of
structures without changing the underlying combinatorics.

The synthetic and external problems are merged into a single benchmark
set. To avoid double–counting identical or trivially transformed
instances, we compute a content hash from the distance information and
basic metadata, and discard duplicates that share the same hash. The
resulting dataset covers a wide range of sizes, metrics, and structural
patterns, and serves as the empirical basis for all subsequent
analyses. For the selector experiments this combined benchmark is then
split at the level of problem identifiers into a training set and a
held–out test set: we shuffle the list of unique \texttt{problem\_id}s
with a fixed random seed and allocate roughly $80\%$ of instances to
training and the remaining $20\%$ to testing. Because the split is done
on the full merged benchmark, both parts contain the same families of
instances and similar size ranges, and no instance appears in both
training and test sets; the 688/166 split reported in
\Cref{sec:autotp} is derived from this partition.

\paragraph{Algorithm portfolio and benchmarking harness.}
The algorithm portfolio comprises exact, approximation, heuristic, and
metaheuristic methods as described in \Cref{sec:algorithms}. A dedicated
benchmarking harness orchestrates the evaluation. For each instance and
each algorithm in the portfolio, it initiates a separate solver run
with a fixed wall–clock budget and an optional memory limit. If a run
exceeds these limits or fails internally, it is terminated and recorded
as an infeasible outcome with an appropriate status code.

To balance statistical robustness against computational cost, we use a
single run per algorithm–instance pair. Stochastic methods such as
simulated annealing or genetic algorithms rely on their internal
randomness to explore the search space; additional repetitions would
primarily reduce Monte Carlo noise rather than change the qualitative
conclusions. Given the large number of instances and solvers, it is
more informative to broaden coverage across instance types and sizes
than to repeat identical configurations many times.

The harness also avoids obviously intractable regimes. For each
algorithm it maintains a short history of recent attempts on
progressively larger instances, ordered by city count. If the last
fifteen runs of a given algorithm all terminate unsuccessfully (i.e.,
are recorded as infeasible) on increasingly large problems, subsequent
instances are automatically treated as infeasible for that algorithm
without attempting explicit runs. Algorithms that successfully solve
any intervening instance reset this history, so that solvers are only
skipped when they have consistently failed in a contiguous regime of
problem sizes.

Every run produces a record containing at least the problem identifier,
algorithm name, status, tour cost (if available), and runtime. These
records are stored in an append–only, line–oriented JSON format and
constitute the raw data used to compute the penalised costs and oracle
choices in \Cref{subsec:evaluation-metrics}.

\paragraph{Selector training and evaluation.}
Selector training is conducted offline on the logged benchmark data.
For each problem we first identify, among all algorithms that completed
within a prescribed selector budget (here 10 seconds), the one with the
lowest tour cost; this defines the “best” algorithm for that instance.
We then pair this label with the corresponding instance features
produced by the feature–extraction module to obtain a training sample
for the learning–based selector. A supervised classifier (a Random
Forest in this work) is trained to predict the best algorithm from
features alone.

Selector evaluation proceeds by replaying the benchmark under the
policies of interest. For each instance we compute the penalised costs
for all algorithms, determine the oracle choice, and then inspect which
algorithm would have been chosen by (i) the rule–based selector,
(ii) the Random Forest selector, and (iii) simple baselines such as
always running the single best algorithm in hindsight. Because all
decisions are simulated on the logged benchmark data, no additional
solver runs are required at evaluation time. The metrics in
\Cref{subsec:evaluation-metrics} are then computed from these replayed
decisions.

\paragraph{Randomness and reproducibility.}
Synthetic instances are generated using a fixed pseudo–random seed for
each configuration, so the benchmark distribution is fully
reproducible given the generation parameters. Feature extraction is a
deterministic function of the instance description. Stochastic solvers
and selectors rely on internal random number generators, but their
outcomes are always recorded in full, together with status codes and
timing information. All subsequent analysis (selector training, regret
computation, and plotting) operates entirely on these logged records,
which makes the empirical results reproducible without rerunning the
underlying algorithms.

All experiments were conducted on a standard multi–core desktop machine
with sufficient memory to hold the largest distance matrices. The main
computational bottleneck is the cumulative runtime of the solver
portfolio on the largest instances; once the benchmark results have
been collected, selector training and evaluation are comparatively
cheap.

\subsection{System Architecture}
\label{subsec:implementation-details}

Conceptually, the AutoTSP system consists of five interacting
components: a canonical instance representation, a feature extraction
module, a portfolio of TSP solvers, a selection layer, and a
benchmarking and logging layer. This section describes these components
at a high level.

\paragraph{Instance representation.}
All TSP instances are represented internally as complete weighted
graphs. Each instance record contains the number of cities, a compact
description of either the coordinates or the distance matrix, and a set
of high–level attributes such as the metric family (Euclidean,
Manhattan, explicit), symmetry (symmetric versus asymmetric), and
problem origin (synthetic or external benchmark). This abstraction
decouples the rest of the system from the details of how raw datasets
are parsed and ensures that every solver sees a uniform interface.

\paragraph{Solver interface and portfolio.}
Every algorithm in the portfolio exposes the same conceptual interface:
given a distance matrix and a time budget, it returns a tour (if found),
its total cost, a status flag indicating success or failure, and the
elapsed runtime. Exact methods, approximation schemes, constructive
heuristics, local search, and metaheuristics are all wrapped behind
this interface. This standardisation allows the benchmarking harness
and the selectors to treat all solvers uniformly, and makes it easy to
extend the portfolio with additional algorithms in future work.

\paragraph{Feature extraction.}
The feature extraction module is responsible for computing the
instance–level descriptors used by the selectors. It operates directly
on the canonical instance representation and produces a small vector of
numeric and Boolean features, such as the number of cities, indicators
of metric structure, measures of spatial dispersion (e.g.\ bounding–box
area, centroid radius), and scale estimates derived from landmark
distances and truncated nearest–neighbour probes. The module is
designed to be significantly cheaper than running any non–trivial
solver, and it records its own runtime so that feature–extraction
overhead can be accounted for in the evaluation.

\paragraph{Selection layer.}
On top of the feature extractor sits the selection layer, which maps
instance features to one of the available solvers. In this report two
selection mechanisms are explored. The first is a rule–based selector
that encodes hand–crafted decision rules over problem size, metric
structure, and time budget to select between exact methods, local
search, and metaheuristics. The second is a learning–based selector,
implemented as a Random Forest classifier trained on benchmark data to
predict, from features alone, which solver will perform best under the
time budget. Both selectors share the same interface and can be swapped
interchangeably in the AutoTSP pipeline.

\paragraph{Benchmarking and logging.}
The benchmarking layer coordinates the interaction between instances,
solvers, and selectors. It feeds instances to the portfolio under
controlled resource limits, collects the resulting outcomes, and
persists them in an append–only, line–oriented JSON format. Each
record is self–contained and includes the instance identifier, selected
algorithm, status, cost, runtime, and any additional metadata produced
by the solver or selector. This design enables all downstream analyses
to be conducted purely from logged data, without rerunning algorithms,
and provides a clear separation between data collection and statistical
evaluation.

The entire system is implemented in Python using standard numerical and
machine–learning libraries (for example NumPy, pandas, scikit–learn,
matplotlib, and seaborn), but the experiments and conclusions presented
in this report depend only on the abstract behaviour of the components
described above.














\newpage
\section{Empirical Analysis of the Algorithm Portfolio}
\label{sec:empirical}

\subsection{Global Performance Overview}
Across the 13\,590 solver invocations in the benchmark log, just over
half (approximately 51\%) completed. Metaheuristics and simple constructive heuristics
have the highest completion rates (simulated annealing $94\%$, simple
nearest neighbour $85\%$, LKH $82\%$), while the exact solvers often
timed out (all four below $30\%$). When they do finish, exact methods,
LKH, and ant colony are essentially optimal (median normalised cost near
$1.0$), whereas purely constructive heuristics deteriorate sharply with
size.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{success_rates.png}
  \caption{Completion rates per algorithm over all $13{,}590$ runs.}
  \label{fig:success_rates}
\end{figure}

Runtime and quality scale predictably with instance size. As shown in
\Cref{fig:combined_results_overall_algorithms}, runtimes rise steeply for exact solvers and
ant colony, remain almost flat for nearest-neighbour style heuristics,
and grow moderately for LKH and simulated annealing. The matching cost
curves mirror this: LKH, Concorde, and Held-Karp stay near the
best-known cost across sizes, ant colony trades speed for slightly
higher cost, and two/three-opt and genetic search drift further from the
optimum on large instances. Completion rates are summarised in
\Cref{fig:success_rates}, highlighting how the global averages are
skewed by the large number of timeouts among the exact methods.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{combined_results_overall_algorithms.png}
  \caption{Global runtime and normalised cost against problem size (mean
  with $\pm1\sigma$ bands over successful runs).}
  \label{fig:combined_results_overall_algorithms}
\end{figure}



\subsection{Pareto Front Analysis by Instance Family}
To study time-quality trade-offs, instances were grouped by problem
family and by size buckets ($5$--$50$, $50$--$200$, $200$--$1000$, and
${>}1000$ cities). For each (family, bucket) we computed median runtime
and normalised cost per algorithm and traced empirical Pareto fronts.
\Cref{fig:pareto-uniform} summarises the Euclidean synthetic cases: for
$5$--$50$ cities the front mixes very fast constructive heuristics
(nearest neighbour and the spatial heuristic) with the exact Held-Karp
baseline; for $50$--$200$ cities LKH and Concorde dominate on quality
while Christofides and the spatial heuristic offer quicker but slightly
worse tours; by $200$--$1000$ cities simulated annealing delivers the
best speed while LKH/Concorde retain the lowest cost.

In the matrix-explicit family the pattern is
starker: LKH sits on the cost frontier across all buckets, simple
nearest neighbour is the fastest but several percent worse, and exact
Held-Karp is competitive only on tiny instances. The TSPLIB Euclidean
instances follow a similar pattern, but beyond $1000$ cities simulated
annealing is the only solver reliably completing within the budget, so
it forms the sole Pareto-efficient point.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\linewidth]{pareto.png}
  \caption{Pareto fronts for uniform Euclidean instances across size
  buckets.}
  \label{fig:pareto-uniform}
\end{figure}

These differences in the Pareto frontiers between instance families (of which there are more than ten in total) emphasise how strongly algorithm performance depends not only on problem size but also on problem type. This further motivates AutoTSP: the selector is explicitly designed to exploit systematic performance variation across instance families.

\subsection{Effect of Instance Features on Algorithm Performance}
Instance-level features (number of cities, metric flag, nearest-neighbour
probe cost, and simple geometric dispersion) were joined with solver
outcomes for the training subset. Averaged over size buckets, exact
methods stay optimal up to around $200$ nodes, metaheuristics remain
near-optimal up to the largest problems, while constructive heuristics
degrade rapidly (mean normalised cost ${>}4$ beyond $200$ nodes). The
selector-style view in \Cref{fig:best-by-size} shows that for $5$--$50$
nodes three-opt and ant colony are most often best, between $50$ and
$1000$ nodes LKH (and occasionally Concorde) dominates, and beyond
$1000$ nodes simulated annealing and the spatial heuristic are the only
consistent winners.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\linewidth]{best_algo_by_size.png}
  \caption{Frequency with which each algorithm attains the best
  normalised cost per size bucket.}
  \label{fig:best-by-size}
\end{figure}

The geometric probes are also predictive:
\Cref{fig:feature-probe} plots normalised cost against the
nearest-neighbour probe. When the probe tour is cheap (within
approximately $2{\times}$ the best scale) exact methods or LKH often
yield the best cost; as the probe cost grows (${>}5{\times}$) the front
collapses to metaheuristics, with exact solvers rarely completing and
heuristics falling far from the optimum. Metric instances favour
Concorde/LKH, while non-metric cases rely more on LKH and the cheaper
three-opt or ant colony tours.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{feature_vs_probe.png}
  \caption{Normalised cost versus nearest-neighbour probe cost coloured
  by algorithm family (training subset).}
  \label{fig:feature-probe}
\end{figure}

\subsection{Discussion of Empirical Findings}
Taken together, the experiments reveal a clear division of labour in the
portfolio. LKH and simulated annealing are the most robust choices: they
scale to the largest instances, achieve near-optimal cost in most
families, and complete reliably. Exact solvers provide the best quality
on small to medium metric instances but fail often and therefore need
guard rails. Constructive heuristics (nearest neighbour, two/three-opt)
are valuable only as extremely fast fallbacks on tiny instances or when
the expensive methods are unsupported.

The strongest trade-offs follow problem size and probe difficulty.
Quality-minded fronts on small problems favour exact methods; for
medium-size metric instances LKH or Concorde give the best balance; on
very large or hard probes metaheuristics dominate because they are the
only algorithms completing within the time budget. These observations
directly motivated the AutoTSP selectors in the next section: simple
rules over size, metric structure, and nearest-neighbour probes capture
most of the variation seen here, and the learning-based selector is
trained to recognise the same transitions (for example, when to back off
from exact search in favour of LKH or simulated annealing). A slightly
surprising outcome is how often simulated annealing is the only method
finishing very large TSPLIB instances, underscoring the need for a
reliable metaheuristic backbone in the portfolio.





















\newpage
\newpage
\section{AutoTSP: Per-Instance Algorithm Selection}
\label{sec:autotp}

\subsection{Design Goals and Constraints}

The design of AutoTSP is guided by three practical goals. First, the
selector should improve average performance over a realistic
distribution of instances, rather than on a few hand–picked examples.
Second, the additional overhead of feature extraction and selection must
be small compared to the cost of solving the instance itself. Third, at
least one selector should be simple and interpretable, so that its
behaviour can be related directly to the qualitative algorithm
comparison in \Cref{sec:empirical}.

All experiments in this chapter adopt a fixed per–instance wall–clock
budget of $10$ seconds for each algorithm, including time spent on
feature extraction and selection. This is deliberately strict: it forces
the selector to choose from algorithms that make different trade–offs
between solution quality and runtime, and it makes timeouts and
infeasible runs a real concern. Performance is therefore assessed using
the penalised cost framework of \Cref{subsec:evaluation-metrics}, with
an oracle selector and several baselines (single best algorithm,
rule–based selector, Random Forest selector) evaluated on the same
held–out test set.

\subsection{Rule-Based Selector}

The rule–based selector provides a transparent baseline that encodes
simple heuristics over instance size, metric structure, and coarse
difficulty indicators. It maps the feature vector produced by the
extractor to one of the portfolio algorithms using a small set of
thresholds.

At a high level, the rules are as follows.
\begin{itemize}[noitemsep,topsep=0.5em]
  \item \textbf{Very small instances.}
    For $n \leq 12$ cities the selector uses the Held–Karp dynamic
    programming algorithm, and for $12 < n \leq 30$ it switches to a
    branch–and–bound solver. On this scale exact methods are fast
    enough to serve as reliable baselines.
  \item \textbf{Very large instances.}
    For $n \geq 4000$ cities the selector falls back to a multi–start
    nearest–neighbour heuristic. This sacrifices some solution quality
    in exchange for predictable runtimes on the largest instances.
  \item \textbf{Non–metric or asymmetric instances.}
    When the feature extractor identifies a non–metric or asymmetric
    distance structure, the selector avoids algorithms that exploit
    Euclidean geometry. For moderately sized non–metric instances it
    prefers a genetic algorithm when the probe features indicate an
    “easy” cost scale, and otherwise uses the Lin–Kernighan–style
    heuristic (LKH), which supports asymmetric cost matrices.
  \item \textbf{Metric instances.}
    For metric problems with up to roughly $150$ cities, the selector
    chooses a 3–opt local–search heuristic; between roughly $150$ and
    $500$ cities it uses iterated local search. Beyond this range it
    prefers LKH when the normalised probe features suggest tightly
    clustered or highly structured instances, and simulated annealing
    otherwise.
\end{itemize}
These rules were derived by combining the qualitative algorithm
taxonomy in \Cref{sec:algorithms} with exploratory benchmarking plots
of cost versus city count. They are intentionally simple, and as will be
seen in \Cref{subsec:selector-eval}, they serve mainly as a strawman
baseline for the learning–based selector.

\subsection{Learning-Based Selector}

The learning–based selector is implemented as a Random Forest classifier
that predicts, from instance features, which algorithm in the portfolio
will perform best under the time budget. Training data are derived from
the benchmark runs described in \Cref{subsec:experimental-protocol}.

\subsubsection{Model Specification and Training}

Each training example corresponds to a single problem instance. For
every problem in the training split we identify the algorithm that
achieves the lowest tour cost among all runs that completed within the
time budget. This algorithm serves as the target label. The input
features are the scale–normalised descriptors produced by the feature
extractor, together with structural indicators:
\begin{itemize}[noitemsep,topsep=0.5em]
  \item number of cities $n$ and a metric flag \texttt{is\_metric};
  \item normalised geometric statistics
    (\texttt{std\_dev\_x\_norm}, \texttt{std\_dev\_y\_norm},
    \texttt{bbox\_area\_norm}, \texttt{centroid\_dispersion\_norm});
  \item scale–invariant distance probes
    (\texttt{landmark\_10\_dist\_norm},
     \texttt{nn\_probe\_cost\_norm},
     \texttt{nn\_probe\_cost\_per\_node});
  \item the nominal time budget and remaining budget, which are fixed at
    training time.
\end{itemize}
The training set is constructed from the portion of the benchmark data
not used for selector evaluation; in total it contains 688 instances
covering all synthetic and external families up to the maximum size
considered. A Random Forest classifier with 100 trees and a fixed
random seed is fit to this data. Predictions at test time are obtained
by feeding a new instance through the feature extractor and then
applying the trained model to the resulting feature vector.

\subsubsection{Feature Importance Analysis}

To understand which features drive the learning–based selector, we
inspect the Gini importances reported by the Random Forest. The most important feature is simply the number of cities, followed by three scale-related probes: the nearest-neighbour probe cost per node, a normalised landmark distance statistic, and the overall nearest-neighbour probe cost. Together, these four features account
for over $75\%$ of the total importance.

Normalised geometric dispersion measures
(\texttt{std\_dev\_x\_norm}, \texttt{std\_dev\_y\_norm},
\texttt{centroid\_dispersion\_norm}) contribute most of the remaining
importance, while the metric flag and budget features play a minor role.
This pattern is consistent with intuition: algorithm performance varies
strongly with problem size and the typical edge and tour scales, and
only secondarily with fine details of the geometry.

For completeness, \Cref{fig:rf-feature-importance} shows a bar plot of
the feature importances of the trained Random Forest model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{rf_feature_importance.png}
    \caption{Feature importances of the Random Forest selector on the training set.}
    \label{fig:rf-feature-importance}
\end{figure}

\subsubsection{Evaluation and Comparison}
\label{subsec:selector-eval}

Selector performance is evaluated on a held–out test set of 166
instances drawn from the combined benchmark distribution. The oracle
selector chooses, for each instance, the algorithm with the lowest
penalised cost among all valid runs. As baselines we include the
rule–based selector, the learning–based selector, and a “single best”
policy that always runs the empirically best fixed algorithm, which in
this setting is simulated annealing.

\Cref{tab:selector-summary} reports summary statistics and summarises the main quality–time trade–off.
All averages are taken over the test instances and costs are
normalised by the oracle cost on each instance so that values close to
1 indicate near–optimal performance.

\begin{table}[H]
    \centering
    \begin{tabular}{@{}lrrrrrrr@{}}
        \toprule
        Selector & Top--1 & Top--2 & Avg.\ norm.\ cost & Avg.\ regret & Median regret & Avg.\ time (s) & Solved \\ \midrule
        Oracle &
        $1.000$ & $0.994$ & $1.000$ & $0$ & $0$ & $1.66$ & $1.00$ \\
        Random Forest &
        $0.633$ & $0.825$ & $1.10$ & $2.96{\times}10^3$ & $0$ & $1.85$ & $0.98$ \\
        Rule–based &
        $0.187$ & $0.361$ & $3.82$ & $3.08{\times}10^4$ & $5.51{\times}10^2$ & $3.71$ & $0.70$ \\
        Single best (SA) &
        $0.108$ & $0.235$ & $1.30$ & $1.68{\times}10^3$ & $1.35{\times}10^2$ & $0.69$ & $1.00$ \\ \bottomrule
    \end{tabular}
    \caption{Selector performance on the held–out test set
    (166 instances). Top--$k$ columns show accuracy with respect to the
    oracle, the average normalised cost is the mean of $ \tilde{q}_S(i) /
    \tilde{q}_{\text{oracle}}(i)$, and regret is measured in absolute
    penalised cost units.}
    \label{tab:selector-summary}
\end{table}

The Random Forest selector matches the oracle exactly on about
63\% of test instances and is within the oracle top two and top three
choices on roughly 83\% and 89\% of instances, respectively. Its median
regret is zero, meaning that on a typical instance it picks an algorithm
whose penalised cost is identical to the oracle best. In terms of
average normalised cost it incurs about a $9.5\%$ overhead over the
oracle ($1.10$ versus $1.00$), while maintaining a high solved fraction
($98\%$).

The single–best policy (always simulated annealing) has much lower
top--$k$ accuracy, but because simulated annealing is a strong general
purpose solver it still achieves a competitive average normalised cost
($1.30$). The rule–based selector, by contrast, is clearly dominated:
it has low top--$k$ accuracy, much higher regret, and a solved fraction
below $70\%$, reflecting its tendency to choose overly ambitious exact
methods on instances where they frequently time out.

Taken together, these results show that the learning–based AutoTSP
selector is an effective approximation to the oracle: it matches the
oracle on most instances, substantially outperforms the hand–crafted
rule–based selector, and improves over the best fixed algorithm in
average normalised cost, at the price of a modest increase in overhead.

\subsection{Cost of Selection vs Benefit}

The evaluation includes the full wall–clock time required for feature
extraction, selection, and running the chosen algorithm. The oracle
policy has an average per–instance time of about $1.66$ seconds, while
the Random Forest selector takes about $1.85$ seconds. The difference
is explained almost entirely by the additional time needed to evaluate
the Random Forest and, on some instances, by choosing slightly heavier
algorithms than the oracle. Compared to the single–best simulated
annealing policy (about $0.69$ seconds on average), the learning–based
selector roughly doubles the average time but reduces the normalised
cost from $1.30$ to $1.10$.

On very small instances exact methods often solve the problem in
milliseconds, and any selection overhead is comparatively large. In this
regime the single–best policy is already close to optimal and there is
little to gain from AutoTSP. The benefits of per–instance selection are
most pronounced in the medium–size range where the portfolio contains
algorithms with very different cost–quality profiles and the Random
Forest can steer instances towards the subset of algorithms that behave
well on that region of the instance space.

\subsection{Case Studies: Selector Behaviour Across Scale}

To gain a deeper intuition into how the selectors navigate the instance space, we examine their decisions as the problem size increases. This analysis helps visualise not just \emph{how well} the selectors perform, but \emph{why} they make specific choices.

\Cref{fig:selector-choices} visualises the specific algorithms chosen by each policy for every instance in the test set, ordered by the number of cities (log scale). The bottom row (Oracle) reveals the "ground truth" of the algorithm landscape. We observe three distinct regimes:
\begin{enumerate}
    \item \textbf{Small scale ($n < 20$):} The Oracle almost exclusively selects exact methods, specifically \texttt{branch\_and\_bound} (pink) and \texttt{held\_karp} (light purple).
    \item \textbf{Medium scale ($20 < n < 1000$):} The landscape becomes heterogeneous. While some constructive heuristics appear, the region is heavily dominated by the Lin-Kernighan heuristic (\texttt{lkh}, orange), which offers an excellent balance of speed and quality for geometric instances in this range.
    \item \textbf{Large scale ($n > 1000$):} As time budgets become tight relative to problem size, the Oracle shifts toward robust metaheuristics, primarily \texttt{simulated\_annealing} (green) and occasionally \texttt{three\_opt} (dark pink).
\end{enumerate}

The \textbf{Random Forest} (second row) faithfully reproduces this complex texture. Crucially, it successfully identifies the "LKH regime" in the middle of the distribution (indicated by the density of orange points), a pattern the manual rules missed entirely.

In contrast, the \textbf{Rule-based} selector (third row) exhibits rigid, "blocky" behaviour. It transitions abruptly from exact methods to \texttt{genetic\_algorithm} (yellow) and \texttt{three\_opt}, completely bypassing the high-performing LKH algorithm. The \textbf{Single Best} policy (top row) is visually monotonous, selecting \texttt{simulated\_annealing} regardless of instance structure.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{selection_algorithms_vs_size.png}
    \caption{Algorithm choices vs city count on the test set. The Random Forest (second row) mimics the textural shifts of the Oracle (bottom row) far better than the rigid Rule-based approach.}
    \label{fig:selector-choices}
\end{figure}

We can quantify the success of these choices by looking at the cumulative accuracy. \Cref{fig:cumulative-accuracy} plots the cumulative fraction of instances where the selector matched the Oracle exactly, sorted by problem size.

The \textbf{Random Forest} (orange line) separates itself early. After an initial learning curve on very small instances—where the penalty for choosing the "wrong" exact solver is high—it climbs steeply between $n=30$ and $n=100$. This corresponds to the regime where distinguishing between LKH, genetic algorithms, and local search is difficult for humans but statistically discernible for the model. It stabilises at a cumulative accuracy of over $60\%$.

The \textbf{Rule-based} selector (green line), however, plateaued below $20\%$. Its inability to select the optimal solver in the complex middle-size regime prevents it from recovering. Similarly, the \textbf{Single Best} algorithm (red line) hovers near zero. While Simulated Annealing is a robust "safe" choice (low regret), it is rarely the \emph{strictly optimal} choice (low accuracy) for any specific instance, usually being beaten by a faster heuristic on small graphs or a more specialised metaheuristic on structured graphs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{selection_cumulative_accuracy.png}
    \caption{Cumulative accuracy vs Oracle by city count. The Random Forest (orange) demonstrates a strong ability to adapt to increasing complexity, whereas the Rule-based and Single Best strategies stagnate.}
    \label{fig:cumulative-accuracy}
\end{figure}



















\newpage
\section{Conclusion and Future Work}
\label{sec:conclusion}

This report has explored per-instance algorithm selection for the
Traveling Salesman Problem through the design and evaluation of
\emph{AutoTSP}. Starting from a heterogeneous benchmark of TSP and
TSP-like instances and a portfolio of diverse solvers, the work has
characterised how TSP algorithm performance varies across instance
families, and used this understanding to construct practical selection
rules that operate under tight time budgets.

The empirical analysis in \Cref{sec:empirical} addressed the first
research question on instance-dependent behaviour. Across synthetic and
real-world instance families and a wide range of sizes, no single
algorithm came close to dominating the quality–runtime trade-off.
Exact methods and specialised solvers such as Concorde and
Lin–Kernighan achieved near-optimal tours on small and medium metric
instances but often timed out or failed on larger or more irregular
problems. Simple constructive heuristics and local search were fast but
lost substantial ground in solution quality as size increased.
Metaheuristics such as simulated annealing and ant colony optimisation
proved to be robust and often formed the only viable Pareto-efficient
choices on large or difficult instances. These findings confirm
Hypothesis~H1 and provide a concrete picture of the “no free lunch”
intuition in a single, well-studied problem domain.

Building on this, \Cref{sec:autotp} studied the value of per-instance
algorithm selection (RQ2) and the potential benefits of learning-based
selection over manual rules (RQ3). The AutoTSP pipeline combines a
lightweight feature extractor with either a rule-based selector or a
Random Forest classifier. Evaluated on a held-out test set using a
failure-aware penalised cost, the learning-based selector matched the
oracle choice on roughly two thirds of instances and was within the
oracle’s top two choices on most of the rest. In terms of average
normalised cost it incurred only a modest overhead over the oracle,
while outperforming both the best fixed algorithm (simulated annealing)
and the rule-based policy. The rule-based selector, although simple and
interpretable, was overly optimistic about exact methods and failed to
adapt to the more subtle transitions between regimes where different
heuristics and metaheuristics are preferable. Taken together, these
results confirm that per-instance selection is beneficial (answering
RQ2) and that a learning-based selector provides a clear improvement
over hand-crafted rules (supporting H3), while also showing that H2,
which posited that the rule-based selector would beat any single fixed
algorithm, is not supported by the data.

At the same time, the work has several limitations. The algorithm
portfolio, while diverse, is still relatively small, and some solvers
are simple Python implementations rather than highly tuned systems.
The instance features are deliberately low-dimensional and hand-designed;
they capture global size and geometry but ignore finer-grained
structure such as local clustering patterns or graph-theoretic
properties. AutoTSP makes a single, one-shot algorithm choice per
instance and does not adapt its decision based on intermediate progress
of the solver. Finally, the selectors are trained and evaluated on a
fixed mixture of synthetic and benchmark instances; their behaviour on
very different application domains remains to be tested.

These limitations suggest several directions for future work:

\begin{itemize}[noitemsep,topsep=0.5em]
  \item \textbf{Richer portfolios and configurations.}
    Extending the portfolio to include more modern or specialised TSP
    solvers, as well as different parameter settings of existing
    algorithms, would turn AutoTSP into a joint selection and
    configuration system. This would allow the selector to choose, for
    example, between short and long runs of the same metaheuristic
    depending on instance difficulty.
  \item \textbf{Improved instance representations.}
    Beyond simple scalar features, one could explore graph-based
    embeddings or neural networks that operate directly on distance
    matrices or coordinates. Such models might capture structural
    patterns that are invisible to the current feature set and further
    close the gap to the oracle selector.
  \item \textbf{Dynamic and multi-step selection.}
    Rather than choosing a single algorithm once, AutoTSP could be
    extended to schedule multiple solvers or to adapt choices based on
    partial progress, for example by terminating an underperforming run
    early and switching to an alternative. This would move from a
    one-shot portfolio decision to a dynamic portfolio or bandit
    setting.
  \item \textbf{Transfer to other problem domains.}
    The methodology used here—building a benchmark, analysing
    instance-dependent performance, and training selectors on cheap
    features—applies beyond the TSP. Extending the approach to related
    problems such as vehicle routing, scheduling, or SAT would test how
    well the ideas transfer and whether common feature patterns or
    selection strategies emerge.
  \item \textbf{User-facing integration.}
    Finally, AutoTSP could be packaged as a small library or service
    that exposes a simple “solve TSP” interface while hiding the
    underlying portfolio and selection logic. This would turn the
    prototype presented in this report into a practical tool for
    researchers and practitioners who want reasonable TSP solutions
    without committing to a single solver.
\end{itemize}

Overall, the report shows that even for a single, classical problem like
the TSP, algorithm selection is both necessary and effective. A modest
amount of structure in the instance distribution, combined with cheap
features and standard supervised learning, is enough to build a selector
that tracks an oracle surprisingly closely and improves over natural
baselines. In this sense, AutoTSP serves as a concrete example of how
meta-optimisation ideas can be made practical at the scale of individual
instances, and as a small step towards more adaptive optimisation
systems in combinatorial settings.

\newpage
\printbibliography

\end{document}
